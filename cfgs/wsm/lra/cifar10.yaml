# General settings
general:
  seed: 2028
  classification: True
  train: True
  dataset: "lra"
  data_path: "./data/lra/cifar10/"
  save_path: null

# Data hyperparameters
data:
  resolution: [32, 32]
  normalize: True

# Model hyperparameters
model:
  model_type: "wsm"
  root_width_size: 64
  root_depth: 1
  root_activation: "sin"
  root_final_activation: null
  init_state_layers: 0
  root_output_dim: 10
  input_prev_data: False
  # positional_encoding: [6, 20]
  positional_encoding: null
  # conv_embedding: null
  conv_embedding: [64, 1024, 0]
  # conv_embedding: [32, 3, 0]
  nb_rnn_layers: 1
  weights_lim: null
  dynamic_tanh_init: null
  time_as_channel: False
  forcing_prob: 0.25
  noise_theta_init: null
  std_lower_bound: 1.0e-4

# Optimizer hyperparameters
optimizer:
  init_lr: 1.0e-5
  gradients_lim: 1.0e-7
  on_plateau:
    factor: 0.95
    min_scale: 1.0e-2
    patience: 20
    cooldown: 0
    rtol: 1.0e-4
    accum_size: 50

# Training/Inference configurations
training:
  nb_epochs: 10000
  batch_size: 512
  autoregressive: False
  stochastic: False
  print_every: 50
  save_every: 50
  valid_every: 50
  val_criterion: "error_rate"
  checkpoint_every: 50
  smooth_inference: True
  nb_recons_loss_steps: null
  use_nll_loss: False
  inference_start: True
